{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzUv3QZpAxzi5rC1mGOkuH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/Personal_Study/blob/main/Deep_Speech2_%ED%85%8C%EC%8A%A4%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sparse_image_warp\n"
      ],
      "metadata": {
        "id": "bgG0VTUeUjcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 RnD at Spoon Radio\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from torch.autograd import Variable\n",
        "# import librosa\n",
        "import random\n",
        "import numpy as np\n",
        "# import scipy.signal\n",
        "import torch\n",
        "# import torchaudio\n",
        "# from torchaudio import transforms\n",
        "# import math\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "def time_warp(spec, W=5):\n",
        "    spec = spec.view(1, spec.shape[0], spec.shape[1])\n",
        "    num_rows = spec.shape[1]\n",
        "    spec_len = spec.shape[2]\n",
        "\n",
        "    y = num_rows // 2\n",
        "    horizontal_line_at_ctr = spec[0][y]\n",
        "    assert len(horizontal_line_at_ctr) == spec_len\n",
        "\n",
        "    point_to_warp = horizontal_line_at_ctr[random.randrange(W, spec_len - W)]\n",
        "    assert isinstance(point_to_warp, torch.Tensor)\n",
        "\n",
        "    # Uniform distribution from (0,W) with chance to be up to W negative\n",
        "    dist_to_warp = random.randrange(-W, W)\n",
        "    src_pts, dest_pts = torch.tensor([[[y, point_to_warp]]]), torch.tensor([[[y, point_to_warp + dist_to_warp]]])\n",
        "    warped_spectro, dense_flows = SparseImageWarp.sparse_image_warp(spec, src_pts, dest_pts)\n",
        "    return warped_spectro.squeeze(3)\n",
        "\n",
        "\n",
        "def freq_mask(spec, F=15, num_masks=1, replace_with_zero=False):\n",
        "    cloned = spec.clone()\n",
        "    num_mel_channels = cloned.shape[1]\n",
        "\n",
        "    for i in range(0, num_masks):\n",
        "        f = random.randrange(0, F)\n",
        "        f_zero = random.randrange(0, num_mel_channels - f)\n",
        "\n",
        "        # avoids randrange error if values are equal and range is empty\n",
        "        if (f_zero == f_zero + f): return cloned\n",
        "\n",
        "        mask_end = random.randrange(f_zero, f_zero + f)\n",
        "        if (replace_with_zero):\n",
        "            cloned[0][f_zero:mask_end] = 0\n",
        "        else:\n",
        "            cloned[0][f_zero:mask_end] = cloned.mean()\n",
        "\n",
        "    return cloned\n",
        "\n",
        "\n",
        "def time_mask(spec, T=15, num_masks=1, replace_with_zero=False):\n",
        "    cloned = spec.clone()\n",
        "    len_spectro = cloned.shape[2]\n",
        "\n",
        "    for i in range(0, num_masks):\n",
        "        t = random.randrange(0, T)\n",
        "        t_zero = random.randrange(0, len_spectro - t)\n",
        "\n",
        "        # avoids randrange error if values are equal and range is empty\n",
        "        if (t_zero == t_zero + t): return cloned\n",
        "\n",
        "        mask_end = random.randrange(t_zero, t_zero + t)\n",
        "        if (replace_with_zero):\n",
        "            cloned[0][:, t_zero:mask_end] = 0\n",
        "        else:\n",
        "            cloned[0][:, t_zero:mask_end] = cloned.mean()\n",
        "    return cloned\n",
        "\n",
        "\n",
        "def sparse_image_warp(img_tensor,\n",
        "                      source_control_point_locations,\n",
        "                      dest_control_point_locations,\n",
        "                      interpolation_order=2,\n",
        "                      regularization_weight=0.0,\n",
        "                      num_boundaries_points=0):\n",
        "    control_point_flows = (dest_control_point_locations - source_control_point_locations)\n",
        "\n",
        "    batch_size, image_height, image_width = img_tensor.shape\n",
        "    grid_locations = get_grid_locations(image_height, image_width)\n",
        "    flattened_grid_locations = torch.tensor(flatten_grid_locations(grid_locations, image_height, image_width))\n",
        "\n",
        "    flattened_flows = interpolate_spline(\n",
        "        dest_control_point_locations,\n",
        "        control_point_flows,\n",
        "        flattened_grid_locations,\n",
        "        interpolation_order,\n",
        "        regularization_weight)\n",
        "\n",
        "    dense_flows = create_dense_flows(flattened_flows, batch_size, image_height, image_width)\n",
        "\n",
        "    warped_image = dense_image_warp(img_tensor, dense_flows)\n",
        "\n",
        "    return warped_image, dense_flows\n",
        "\n",
        "\n",
        "def get_grid_locations(image_height, image_width):\n",
        "    \"\"\"Wrapper for np.meshgrid.\"\"\"\n",
        "\n",
        "    y_range = np.linspace(0, image_height - 1, image_height)\n",
        "    x_range = np.linspace(0, image_width - 1, image_width)\n",
        "    y_grid, x_grid = np.meshgrid(y_range, x_range, indexing='ij')\n",
        "    return np.stack((y_grid, x_grid), -1)\n",
        "\n",
        "\n",
        "def flatten_grid_locations(grid_locations, image_height, image_width):\n",
        "    return np.reshape(grid_locations, [image_height * image_width, 2])\n",
        "\n",
        "\n",
        "def create_dense_flows(flattened_flows, batch_size, image_height, image_width):\n",
        "    # possibly .view\n",
        "    return torch.reshape(flattened_flows, [batch_size, image_height, image_width, 2])\n",
        "\n",
        "\n",
        "def interpolate_spline(train_points, train_values, query_points, order, regularization_weight=0.0, ):\n",
        "    # First, fit the spline to the observed data.\n",
        "    w, v = solve_interpolation(train_points, train_values, order, regularization_weight)\n",
        "    # Then, evaluate the spline at the query locations.\n",
        "    query_values = apply_interpolation(query_points, train_points, w, v, order)\n",
        "\n",
        "    return query_values\n",
        "\n",
        "\n",
        "def solve_interpolation(train_points, train_values, order, regularization_weight):\n",
        "    b, n, d = train_points.shape\n",
        "    k = train_values.shape[-1]\n",
        "\n",
        "    # First, rename variables so that the notation (c, f, w, v, A, B, etc.)\n",
        "    # follows https://en.wikipedia.org/wiki/Polyharmonic_spline.\n",
        "    # To account for python style guidelines we use\n",
        "    # matrix_a for A and matrix_b for B.\n",
        "\n",
        "    c = train_points\n",
        "    f = train_values.float()\n",
        "\n",
        "    matrix_a = phi(cross_squared_distance_matrix(c, c), order).unsqueeze(0)  # [b, n, n]\n",
        "    #     if regularization_weight > 0:\n",
        "    #         batch_identity_matrix = array_ops.expand_dims(\n",
        "    #           linalg_ops.eye(n, dtype=c.dtype), 0)\n",
        "    #         matrix_a += regularization_weight * batch_identity_matrix\n",
        "\n",
        "    # Append ones to the feature values for the bias term in the linear model.\n",
        "    ones = torch.ones(1, dtype=train_points.dtype).view([-1, 1, 1])\n",
        "    matrix_b = torch.cat((c, ones), 2).float()  # [b, n, d + 1]\n",
        "\n",
        "    # [b, n + d + 1, n]\n",
        "    left_block = torch.cat((matrix_a, torch.transpose(matrix_b, 2, 1)), 1)\n",
        "\n",
        "    num_b_cols = matrix_b.shape[2]  # d + 1\n",
        "\n",
        "    # In Tensorflow, zeros are used here. Pytorch gesv fails with zeros for some reason we don't understand.\n",
        "    # So instead we use very tiny randn values (variance of one, zero mean) on one side of our multiplication.\n",
        "    lhs_zeros = torch.randn((b, num_b_cols, num_b_cols)) / 1e10\n",
        "    right_block = torch.cat((matrix_b, lhs_zeros),\n",
        "                            1)  # [b, n + d + 1, d + 1]\n",
        "    lhs = torch.cat((left_block, right_block),\n",
        "                    2)  # [b, n + d + 1, n + d + 1]\n",
        "\n",
        "    rhs_zeros = torch.zeros((b, d + 1, k), dtype=train_points.dtype).float()\n",
        "    rhs = torch.cat((f, rhs_zeros), 1)  # [b, n + d + 1, k]\n",
        "\n",
        "    # Then, solve the linear system and unpack the results.\n",
        "    X, LU = torch.solve(rhs, lhs)\n",
        "    w = X[:, :n, :]\n",
        "    v = X[:, n:, :]\n",
        "\n",
        "    return w, v\n",
        "\n",
        "\n",
        "def cross_squared_distance_matrix(x, y):\n",
        "    \"\"\"Pairwise squared distance between two (batch) matrices' rows (2nd dim).\n",
        "        Computes the pairwise distances between rows of x and rows of y\n",
        "        Args:\n",
        "        x: [batch_size, n, d] float `Tensor`\n",
        "        y: [batch_size, m, d] float `Tensor`\n",
        "        Returns:\n",
        "        squared_dists: [batch_size, n, m] float `Tensor`, where\n",
        "        squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2\n",
        "    \"\"\"\n",
        "    x_norm_squared = torch.sum(torch.mul(x, x))\n",
        "    y_norm_squared = torch.sum(torch.mul(y, y))\n",
        "\n",
        "    x_y_transpose = torch.matmul(x.squeeze(0), y.squeeze(0).transpose(0, 1))\n",
        "\n",
        "    # squared_dists[b,i,j] = ||x_bi - y_bj||^2 = x_bi'x_bi- 2x_bi'x_bj + x_bj'x_bj\n",
        "    squared_dists = x_norm_squared - 2 * x_y_transpose + y_norm_squared\n",
        "\n",
        "    return squared_dists.float()\n",
        "\n",
        "\n",
        "def phi(r, order):\n",
        "    \"\"\"Coordinate-wise nonlinearity used to define the order of the interpolation.\n",
        "    See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition.\n",
        "    Args:\n",
        "    r: input op\n",
        "    order: interpolation order\n",
        "    Returns:\n",
        "    phi_k evaluated coordinate-wise on r, for k = r\n",
        "    \"\"\"\n",
        "    EPSILON = torch.tensor(1e-10)\n",
        "    # using EPSILON prevents log(0), sqrt0), etc.\n",
        "    # sqrt(0) is well-defined, but its gradient is not\n",
        "    if order == 1:\n",
        "        r = torch.max(r, EPSILON)\n",
        "        r = torch.sqrt(r)\n",
        "        return r\n",
        "    elif order == 2:\n",
        "        return 0.5 * r * torch.log(torch.max(r, EPSILON))\n",
        "    elif order == 4:\n",
        "        return 0.5 * torch.square(r) * torch.log(torch.max(r, EPSILON))\n",
        "    elif order % 2 == 0:\n",
        "        r = torch.max(r, EPSILON)\n",
        "        return 0.5 * torch.pow(r, 0.5 * order) * torch.log(r)\n",
        "    else:\n",
        "        r = torch.max(r, EPSILON)\n",
        "        return torch.pow(r, 0.5 * order)\n",
        "\n",
        "\n",
        "def apply_interpolation(query_points, train_points, w, v, order):\n",
        "    \"\"\"Apply polyharmonic interpolation model to data.\n",
        "    Given coefficients w and v for the interpolation model, we evaluate\n",
        "    interpolated function values at query_points.\n",
        "    Args:\n",
        "    query_points: `[b, m, d]` x values to evaluate the interpolation at\n",
        "    train_points: `[b, n, d]` x values that act as the interpolation centers\n",
        "                    ( the c variables in the wikipedia article)\n",
        "    w: `[b, n, k]` weights on each interpolation center\n",
        "    v: `[b, d, k]` weights on each input dimension\n",
        "    order: order of the interpolation\n",
        "    Returns:\n",
        "    Polyharmonic interpolation evaluated at points defined in query_points.\n",
        "    \"\"\"\n",
        "    query_points = query_points.unsqueeze(0)\n",
        "    # First, compute the contribution from the rbf term.\n",
        "    pairwise_dists = cross_squared_distance_matrix(query_points.float(), train_points.float())\n",
        "    phi_pairwise_dists = phi(pairwise_dists, order)\n",
        "\n",
        "    rbf_term = torch.matmul(phi_pairwise_dists, w)\n",
        "\n",
        "    # Then, compute the contribution from the linear term.\n",
        "    # Pad query_points with ones, for the bias term in the linear model.\n",
        "    ones = torch.ones_like(query_points[..., :1])\n",
        "    query_points_pad = torch.cat((\n",
        "        query_points,\n",
        "        ones\n",
        "    ), 2).float()\n",
        "    linear_term = torch.matmul(query_points_pad, v)\n",
        "\n",
        "    return rbf_term + linear_term\n",
        "\n",
        "\n",
        "def dense_image_warp(image, flow):\n",
        "    \"\"\"Image warping using per-pixel flow vectors.\n",
        "    Apply a non-linear warp to the image, where the warp is specified by a dense\n",
        "    flow field of offset vectors that define the correspondences of pixel values\n",
        "    in the output image back to locations in the  source image. Specifically, the\n",
        "    pixel value at output[b, j, i, c] is\n",
        "    images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c].\n",
        "    The locations specified by this formula do not necessarily map to an int\n",
        "    index. Therefore, the pixel value is obtained by bilinear\n",
        "    interpolation of the 4 nearest pixels around\n",
        "    (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside\n",
        "    of the image, we use the nearest pixel values at the image boundary.\n",
        "    Args:\n",
        "    image: 4-D float `Tensor` with shape `[batch, height, width, channels]`.\n",
        "    flow: A 4-D float `Tensor` with shape `[batch, height, width, 2]`.\n",
        "    name: A name for the operation (optional).\n",
        "    Note that image and flow can be of type tf.half, tf.float32, or tf.float64,\n",
        "    and do not necessarily have to be the same type.\n",
        "    Returns:\n",
        "    A 4-D float `Tensor` with shape`[batch, height, width, channels]`\n",
        "    and same type as input image.\n",
        "    Raises:\n",
        "    ValueError: if height < 2 or width < 2 or the inputs have the wrong number\n",
        "    of dimensions.\n",
        "    \"\"\"\n",
        "    image = image.unsqueeze(3)  # add a single channel dimension to image tensor\n",
        "    batch_size, height, width, channels = image.shape\n",
        "\n",
        "    # The flow is defined on the image grid. Turn the flow into a list of query\n",
        "    # points in the grid space.\n",
        "    grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(width), torch.arange(height))\n",
        "\n",
        "    stacked_grid = torch.stack((grid_y, grid_x), dim=2).float()\n",
        "\n",
        "    batched_grid = stacked_grid.unsqueeze(-1).permute(3, 1, 0, 2)\n",
        "\n",
        "    query_points_on_grid = batched_grid - flow\n",
        "    query_points_flattened = torch.reshape(query_points_on_grid,\n",
        "                                           [batch_size, height * width, 2])\n",
        "    # Compute values at the query points, then reshape the result back to the\n",
        "    # image grid.\n",
        "    interpolated = interpolate_bilinear(image, query_points_flattened)\n",
        "    interpolated = torch.reshape(interpolated,\n",
        "                                 [batch_size, height, width, channels])\n",
        "    return interpolated\n",
        "\n",
        "\n",
        "def interpolate_bilinear(grid,\n",
        "                         query_points,\n",
        "                         name='interpolate_bilinear',\n",
        "                         indexing='ij'):\n",
        "    \"\"\"Similar to Matlab's interp2 function.\n",
        "    Finds values for query points on a grid using bilinear interpolation.\n",
        "    Args:\n",
        "    grid: a 4-D float `Tensor` of shape `[batch, height, width, channels]`.\n",
        "    query_points: a 3-D float `Tensor` of N points with shape `[batch, N, 2]`.\n",
        "    name: a name for the operation (optional).\n",
        "    indexing: whether the query points are specified as row and column (ij),\n",
        "      or Cartesian coordinates (xy).\n",
        "    Returns:\n",
        "    values: a 3-D `Tensor` with shape `[batch, N, channels]`\n",
        "    Raises:\n",
        "    ValueError: if the indexing mode is invalid, or if the shape of the inputs\n",
        "      invalid.\n",
        "    \"\"\"\n",
        "    if indexing != 'ij' and indexing != 'xy':\n",
        "        raise ValueError('Indexing mode must be \\'ij\\' or \\'xy\\'')\n",
        "\n",
        "    shape = grid.shape\n",
        "    if len(shape) != 4:\n",
        "        msg = 'Grid must be 4 dimensional. Received size: '\n",
        "        raise ValueError(msg + str(grid.shape))\n",
        "\n",
        "    batch_size, height, width, channels = grid.shape\n",
        "\n",
        "    shape = [batch_size, height, width, channels]\n",
        "    query_type = query_points.dtype\n",
        "    grid_type = grid.dtype\n",
        "\n",
        "    num_queries = query_points.shape[1]\n",
        "\n",
        "    alphas = []\n",
        "    floors = []\n",
        "    ceils = []\n",
        "    index_order = [0, 1] if indexing == 'ij' else [1, 0]\n",
        "    unstacked_query_points = query_points.unbind(2)\n",
        "\n",
        "    for dim in index_order:\n",
        "        queries = unstacked_query_points[dim]\n",
        "\n",
        "        size_in_indexing_dimension = shape[dim + 1]\n",
        "\n",
        "        # max_floor is size_in_indexing_dimension - 2 so that max_floor + 1\n",
        "        # is still a valid index into the grid.\n",
        "        max_floor = torch.tensor(size_in_indexing_dimension - 2, dtype=query_type)\n",
        "        min_floor = torch.tensor(0.0, dtype=query_type)\n",
        "        maxx = torch.max(min_floor, torch.floor(queries))\n",
        "        floor = torch.min(maxx, max_floor)\n",
        "        int_floor = floor.long()\n",
        "        floors.append(int_floor)\n",
        "        ceil = int_floor + 1\n",
        "        ceils.append(ceil)\n",
        "\n",
        "        # alpha has the same type as the grid, as we will directly use alpha\n",
        "        # when taking linear combinations of pixel values from the image.\n",
        "        alpha = queries.clone().detach() - floor.clone().detach()\n",
        "        min_alpha = torch.tensor(0.0, dtype=grid_type)\n",
        "        max_alpha = torch.tensor(1.0, dtype=grid_type)\n",
        "        alpha = torch.min(torch.max(min_alpha, alpha), max_alpha)\n",
        "\n",
        "        # Expand alpha to [b, n, 1] so we can use broadcasting\n",
        "        # (since the alpha values don't depend on the channel).\n",
        "        alpha = torch.unsqueeze(alpha, 2)\n",
        "        alphas.append(alpha)\n",
        "\n",
        "    flattened_grid = torch.reshape(\n",
        "        grid, [batch_size * height * width, channels])\n",
        "    batch_offsets = torch.reshape(\n",
        "        torch.arange(batch_size) * height * width, [batch_size, 1])\n",
        "\n",
        "    # This wraps array_ops.gather. We reshape the image data such that the\n",
        "    # batch, y, and x coordinates are pulled into the first dimension.\n",
        "    # Then we gather. Finally, we reshape the output back. It's possible this\n",
        "    # code would be made simpler by using array_ops.gather_nd.\n",
        "    def gather(y_coords, x_coords, name):\n",
        "        linear_coordinates = batch_offsets + y_coords * width + x_coords\n",
        "        gathered_values = torch.gather(flattened_grid.t(), 1, linear_coordinates)\n",
        "        return torch.reshape(gathered_values,\n",
        "                             [batch_size, num_queries, channels])\n",
        "\n",
        "    # grab the pixel values in the 4 corners around each query point\n",
        "    top_left = gather(floors[0], floors[1], 'top_left')\n",
        "    top_right = gather(floors[0], ceils[1], 'top_right')\n",
        "    bottom_left = gather(ceils[0], floors[1], 'bottom_left')\n",
        "    bottom_right = gather(ceils[0], ceils[1], 'bottom_right')\n",
        "\n",
        "    interp_top = alphas[1] * (top_right - top_left) + top_left\n",
        "    interp_bottom = alphas[1] * (bottom_right - bottom_left) + bottom_left\n",
        "    interp = alphas[0] * (interp_bottom - interp_top) + interp_top\n",
        "\n",
        "    return interp"
      ],
      "metadata": {
        "id": "Phmi1zWnUjQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spec_augment\n"
      ],
      "metadata": {
        "id": "rYnE1O05UQai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MaHGU_8TDUS"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 RnD at Spoon Radio\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"SpecAugment Implementation for Tensorflow.\n",
        "Related paper : https://arxiv.org/pdf/1904.08779.pdf\n",
        "In this paper, show summarized parameters by each open datasets in Tabel 1.\n",
        "-----------------------------------------\n",
        "Policy | W  | F  | m_F |  T  |  p  | m_T\n",
        "-----------------------------------------\n",
        "None   |  0 |  0 |  -  |  0  |  -  |  -\n",
        "-----------------------------------------\n",
        "LB     | 80 | 27 |  1  | 100 | 1.0 | 1\n",
        "-----------------------------------------\n",
        "LD     | 80 | 27 |  2  | 100 | 1.0 | 2\n",
        "-----------------------------------------\n",
        "SM     | 40 | 15 |  2  |  70 | 0.2 | 2\n",
        "-----------------------------------------\n",
        "SS     | 40 | 27 |  2  |  70 | 0.2 | 2\n",
        "-----------------------------------------\n",
        "LB : LibriSpeech basic\n",
        "LD : LibriSpeech double\n",
        "SM : Switchboard mild\n",
        "SS : Switchboard strong\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 데이터 증강인듯?\n",
        "\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from .sparse_image_warp import sparse_image_warp\n",
        "import torch\n",
        "\n",
        "\n",
        "def time_warp(spec, W=5):\n",
        "    num_rows = spec.shape[1]\n",
        "    spec_len = spec.shape[2]\n",
        "\n",
        "    y = num_rows // 2\n",
        "    horizontal_line_at_ctr = spec[0][y]\n",
        "    # assert len(horizontal_line_at_ctr) == spec_len\n",
        "\n",
        "    point_to_warp = horizontal_line_at_ctr[random.randrange(W, spec_len-W)]\n",
        "    # assert isinstance(point_to_warp, torch.Tensor)\n",
        "\n",
        "    # Uniform distribution from (0,W) with chance to be up to W negative\n",
        "    dist_to_warp = random.randrange(-W, W)\n",
        "    src_pts = torch.tensor([[[y, point_to_warp]]])\n",
        "    dest_pts = torch.tensor([[[y, point_to_warp + dist_to_warp]]])\n",
        "    warped_spectro, dense_flows = sparse_image_warp(spec, src_pts, dest_pts)\n",
        "\n",
        "    return warped_spectro.squeeze(3)\n",
        "\n",
        "\n",
        "def spec_augment(mel_spectrogram, time_warping_para=40, frequency_masking_para=27,\n",
        "                 time_masking_para=70, frequency_mask_num=1, time_mask_num=1):\n",
        "    \"\"\"Spec augmentation Calculation Function.\n",
        "    'SpecAugment' have 3 steps for audio data augmentation.\n",
        "    first step is time warping using Tensorflow's image_sparse_warp function.\n",
        "    Second step is frequency masking, last step is time masking.\n",
        "    # Arguments:\n",
        "      mel_spectrogram(numpy array): audio file path of you want to warping and masking.\n",
        "      time_warping_para(float): Augmentation parameter, \"time warp parameter W\".\n",
        "        If none, default = 40.\n",
        "      frequency_masking_para(float): Augmentation parameter, \"frequency mask parameter F\"\n",
        "        If none, default = 27.\n",
        "      time_masking_para(float): Augmentation parameter, \"time mask parameter T\"\n",
        "        If none, default = 70.\n",
        "      frequency_mask_num(float): number of frequency masking lines, \"m_F\".\n",
        "        If none, default = 1.\n",
        "      time_mask_num(float): number of time masking lines, \"m_T\".\n",
        "        If none, default = 1.\n",
        "    # Returns\n",
        "      mel_spectrogram(numpy array): warped and masked mel spectrogram.\n",
        "    \"\"\"\n",
        "    mel_spectrogram = mel_spectrogram.unsqueeze(0)\n",
        "\n",
        "    v = mel_spectrogram.shape[1]\n",
        "    tau = mel_spectrogram.shape[2]\n",
        "\n",
        "    # Step 1 : Time warping\n",
        "    warped_mel_spectrogram = time_warp(mel_spectrogram)\n",
        "\n",
        "    # Step 2 : Frequency masking\n",
        "    for i in range(frequency_mask_num):\n",
        "        f = np.random.uniform(low=0.0, high=frequency_masking_para)\n",
        "        f = int(f)\n",
        "        if v - f < 0:\n",
        "            continue\n",
        "        f0 = random.randint(0, v-f)\n",
        "        warped_mel_spectrogram[:, f0:f0+f, :] = 0\n",
        "\n",
        "    # Step 3 : Time masking\n",
        "    for i in range(time_mask_num):\n",
        "        t = np.random.uniform(low=0.0, high=time_masking_para)\n",
        "        t = int(t)\n",
        "        if tau - t < 0:\n",
        "            continue\n",
        "        t0 = random.randint(0, tau-t)\n",
        "        warped_mel_spectrogram[:, :, t0:t0+t] = 0\n",
        "\n",
        "    return warped_mel_spectrogram.squeeze()\n",
        "\n",
        "\n",
        "def visualization_spectrogram(mel_spectrogram, title):\n",
        "    \"\"\"visualizing result of SpecAugment\n",
        "    # Arguments:\n",
        "      mel_spectrogram(ndarray): mel_spectrogram to visualize.\n",
        "      title(String): plot figure's title\n",
        "    \"\"\"\n",
        "    # Show mel-spectrogram using librosa's specshow.\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(librosa.power_to_db(mel_spectrogram[0, :, :], ref=np.max), y_axis='mel', fmax=8000, x_axis='time')\n",
        "    # plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data_loader"
      ],
      "metadata": {
        "id": "LXF07AQlUyvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import soundfile as sf\n",
        "import sox\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Sampler, DistributedSampler, DataLoader\n",
        "\n",
        "from deepspeech_pytorch.loader.spec_augment import spec_augment\n",
        "\n",
        "windows = {\n",
        "    'hamming': scipy.signal.hamming,\n",
        "    'hann': scipy.signal.hann,\n",
        "    'blackman': scipy.signal.blackman,\n",
        "    'bartlett': scipy.signal.bartlett\n",
        "}\n",
        "\n",
        "\n",
        "def load_audio(path):\n",
        "    sound, sample_rate = sf.read(path, dtype='int16')\n",
        "    # TODO this should be 32768.0 to get twos-complement range.\n",
        "    # TODO the difference is negligible but should be fixed for new models.\n",
        "    sound = sound.astype('float32') / 32767  # normalize audio\n",
        "    if len(sound.shape) > 1:\n",
        "        if sound.shape[1] == 1:\n",
        "            sound = sound.squeeze()\n",
        "        else:\n",
        "            sound = sound.mean(axis=1)  # multiple channels, average\n",
        "    return sound\n",
        "\n",
        "\n",
        "class AudioParser(object):\n",
        "    def parse_transcript(self, transcript_path):\n",
        "        \"\"\"\n",
        "        :param transcript_path: Path where transcript is stored from the manifest file\n",
        "        :return: Transcript in training/testing format\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def parse_audio(self, audio_path):\n",
        "        \"\"\"\n",
        "        :param audio_path: Path where audio is stored from the manifest file\n",
        "        :return: Audio in training/testing format\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class NoiseInjection(object):\n",
        "    def __init__(self,\n",
        "                 path=None,\n",
        "                 sample_rate=16000,\n",
        "                 noise_levels=(0, 0.5)):\n",
        "        \"\"\"\n",
        "        Adds noise to an input signal with specific SNR. Higher the noise level, the more noise added.\n",
        "        Modified code from https://github.com/willfrey/audio/blob/master/torchaudio/transforms.py\n",
        "        \"\"\"\n",
        "        if not os.path.exists(path):\n",
        "            print(\"Directory doesn't exist: {}\".format(path))\n",
        "            raise IOError\n",
        "        self.paths = path is not None and librosa.util.find_files(path)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.noise_levels = noise_levels\n",
        "\n",
        "    def inject_noise(self, data):\n",
        "        noise_path = np.random.choice(self.paths)\n",
        "        noise_level = np.random.uniform(*self.noise_levels)\n",
        "        return self.inject_noise_sample(data, noise_path, noise_level)\n",
        "\n",
        "    def inject_noise_sample(self, data, noise_path, noise_level):\n",
        "        noise_len = sox.file_info.duration(noise_path)\n",
        "        data_len = len(data) / self.sample_rate\n",
        "        noise_start = np.random.rand() * (noise_len - data_len)\n",
        "        noise_end = noise_start + data_len\n",
        "        noise_dst = audio_with_sox(noise_path, self.sample_rate, noise_start, noise_end)\n",
        "        assert len(data) == len(noise_dst)\n",
        "        noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)\n",
        "        data_energy = np.sqrt(data.dot(data) / data.size)\n",
        "        data += noise_level * noise_dst * data_energy / noise_energy\n",
        "        return data\n",
        "\n",
        "\n",
        "class SpectrogramParser(AudioParser):\n",
        "    def __init__(self, audio_conf, normalize=False, speed_volume_perturb=False, spec_augment=False):\n",
        "        \"\"\"\n",
        "        Parses audio file into spectrogram with optional normalization and various augmentations\n",
        "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
        "        :param normalize(default False):  Apply standard mean and deviation normalization to audio tensor\n",
        "        :param speed_volume_perturb(default False): Apply random tempo and gain perturbations\n",
        "        :param spec_augment(default False): Apply simple spectral augmentation to mel spectograms\n",
        "        \"\"\"\n",
        "        super(SpectrogramParser, self).__init__()\n",
        "        self.window_stride = audio_conf['window_stride']\n",
        "        self.window_size = audio_conf['window_size']\n",
        "        self.sample_rate = audio_conf['sample_rate']\n",
        "        self.window = windows.get(audio_conf['window'], windows['hamming'])\n",
        "        self.normalize = normalize\n",
        "        self.speed_volume_perturb = speed_volume_perturb\n",
        "        self.spec_augment = spec_augment\n",
        "        self.noiseInjector = NoiseInjection(audio_conf['noise_dir'], self.sample_rate,\n",
        "                                            audio_conf['noise_levels']) if audio_conf.get(\n",
        "            'noise_dir') is not None else None\n",
        "        self.noise_prob = audio_conf.get('noise_prob')\n",
        "\n",
        "    def parse_audio(self, audio_path):\n",
        "        if self.speed_volume_perturb:\n",
        "            y = load_randomly_augmented_audio(audio_path, self.sample_rate)\n",
        "        else:\n",
        "            y = load_audio(audio_path)\n",
        "        if self.noiseInjector:\n",
        "            add_noise = np.random.binomial(1, self.noise_prob)\n",
        "            if add_noise:\n",
        "                y = self.noiseInjector.inject_noise(y)\n",
        "        n_fft = int(self.sample_rate * self.window_size)\n",
        "        win_length = n_fft\n",
        "        hop_length = int(self.sample_rate * self.window_stride)\n",
        "        # STFT\n",
        "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
        "                         win_length=win_length, window=self.window)\n",
        "        spect, phase = librosa.magphase(D)\n",
        "        # S = log(S+1)\n",
        "        spect = np.log1p(spect)\n",
        "        spect = torch.FloatTensor(spect)\n",
        "        if self.normalize:\n",
        "            mean = spect.mean()\n",
        "            std = spect.std()\n",
        "            spect.add_(-mean)\n",
        "            spect.div_(std)\n",
        "\n",
        "        if self.spec_augment:\n",
        "            spect = spec_augment(spect)\n",
        "\n",
        "        return spect\n",
        "\n",
        "    def parse_transcript(self, transcript_path):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class SpectrogramDataset(Dataset, SpectrogramParser):\n",
        "    def __init__(self, audio_conf, manifest_filepath, labels, normalize=False, speed_volume_perturb=False,\n",
        "                 spec_augment=False):\n",
        "        \"\"\"\n",
        "        Dataset that loads tensors via a csv containing file paths to audio files and transcripts separated by\n",
        "        a comma. Each new line is a different sample. Example below:\n",
        "\n",
        "        /path/to/audio.wav,/path/to/audio.txt\n",
        "        ...\n",
        "\n",
        "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
        "        :param manifest_filepath: Path to manifest csv as describe above\n",
        "        :param labels: String containing all the possible characters to map to\n",
        "        :param normalize: Apply standard mean and deviation normalization to audio tensor\n",
        "        :param speed_volume_perturb(default False): Apply random tempo and gain perturbations\n",
        "        :param spec_augment(default False): Apply simple spectral augmentation to mel spectograms\n",
        "        \"\"\"\n",
        "        with open(manifest_filepath) as f:\n",
        "            ids = f.readlines()\n",
        "        ids = [x.strip().split(',') for x in ids]\n",
        "        self.ids = ids\n",
        "        self.size = len(ids)\n",
        "        self.labels_map = dict([(labels[i], i) for i in range(len(labels))])\n",
        "        super(SpectrogramDataset, self).__init__(audio_conf, normalize, speed_volume_perturb, spec_augment)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.ids[index]\n",
        "        audio_path, transcript_path = sample[0], sample[1]\n",
        "        spect = self.parse_audio(audio_path)\n",
        "        transcript = self.parse_transcript(transcript_path)\n",
        "        return spect, transcript\n",
        "\n",
        "    def parse_transcript(self, transcript_path):\n",
        "        with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
        "            transcript = transcript_file.read().replace('\\n', '')\n",
        "        transcript = list(filter(None, [self.labels_map.get(x) for x in list(transcript)]))\n",
        "        return transcript\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    def func(p):\n",
        "        return p[0].size(1)\n",
        "\n",
        "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
        "    longest_sample = max(batch, key=func)[0]\n",
        "    freq_size = longest_sample.size(0)\n",
        "    minibatch_size = len(batch)\n",
        "    max_seqlength = longest_sample.size(1)\n",
        "    inputs = torch.zeros(minibatch_size, 1, freq_size, max_seqlength)\n",
        "    input_percentages = torch.FloatTensor(minibatch_size)\n",
        "    target_sizes = torch.IntTensor(minibatch_size)\n",
        "    targets = []\n",
        "    for x in range(minibatch_size):\n",
        "        sample = batch[x]\n",
        "        tensor = sample[0]\n",
        "        target = sample[1]\n",
        "        seq_length = tensor.size(1)\n",
        "        inputs[x][0].narrow(1, 0, seq_length).copy_(tensor)\n",
        "        input_percentages[x] = seq_length / float(max_seqlength)\n",
        "        target_sizes[x] = len(target)\n",
        "        targets.extend(target)\n",
        "    targets = torch.IntTensor(targets)\n",
        "    return inputs, targets, input_percentages, target_sizes\n",
        "\n",
        "\n",
        "class AudioDataLoader(DataLoader):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a data loader for AudioDatasets.\n",
        "        \"\"\"\n",
        "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
        "        self.collate_fn = _collate_fn\n",
        "\n",
        "\n",
        "class DSRandomSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Implementation of a Random Sampler for sampling the dataset.\n",
        "    Added to ensure we reset the start index when an epoch is finished.\n",
        "    This is essential since we support saving/loading state during an epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, batch_size=1, start_index=0):\n",
        "        super().__init__(data_source=dataset)\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.start_index = start_index\n",
        "        self.batch_size = batch_size\n",
        "        ids = list(range(len(self.dataset)))\n",
        "        self.bins = [ids[i:i + self.batch_size] for i in range(0, len(ids), self.batch_size)]\n",
        "\n",
        "    def __iter__(self):\n",
        "        # deterministically shuffle based on epoch\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(self.epoch)\n",
        "        indices = (\n",
        "            torch.randperm(len(self.bins) - self.start_index, generator=g)\n",
        "                .add(self.start_index)\n",
        "                .tolist()\n",
        "        )\n",
        "        for x in indices:\n",
        "            batch_ids = self.bins[x]\n",
        "            np.random.shuffle(batch_ids)\n",
        "            yield batch_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.bins) - self.start_index\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        self.epoch = epoch\n",
        "\n",
        "    def reset_training_step(self, training_step):\n",
        "        self.start_index = training_step\n",
        "\n",
        "\n",
        "class DSElasticDistributedSampler(DistributedSampler):\n",
        "    \"\"\"\n",
        "    Overrides the ElasticDistributedSampler to ensure we reset the start index when an epoch is finished.\n",
        "    This is essential since we support saving/loading state during an epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, num_replicas=None, rank=None, start_index=0, batch_size=1):\n",
        "        super().__init__(dataset=dataset, num_replicas=num_replicas, rank=rank)\n",
        "        self.start_index = start_index\n",
        "        self.batch_size = batch_size\n",
        "        ids = list(range(len(dataset)))\n",
        "        self.bins = [ids[i:i + self.batch_size] for i in range(0, len(ids), self.batch_size)]\n",
        "        self.num_samples = int(\n",
        "            math.ceil(float(len(self.bins) - self.start_index) / self.num_replicas)\n",
        "        )\n",
        "        self.total_size = self.num_samples * self.num_replicas\n",
        "\n",
        "    def __iter__(self):\n",
        "        # deterministically shuffle based on epoch\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(self.epoch)\n",
        "        indices = (\n",
        "            torch.randperm(len(self.bins) - self.start_index, generator=g)\n",
        "                .add(self.start_index)\n",
        "                .tolist()\n",
        "        )\n",
        "\n",
        "        # add extra samples to make it evenly divisible\n",
        "        indices += indices[: (self.total_size - len(indices))]\n",
        "        assert len(indices) == self.total_size\n",
        "\n",
        "        # subsample\n",
        "        indices = indices[self.rank: self.total_size: self.num_replicas]\n",
        "        assert len(indices) == self.num_samples\n",
        "        for x in indices:\n",
        "            batch_ids = self.bins[x]\n",
        "            np.random.shuffle(batch_ids)\n",
        "            yield batch_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def reset_training_step(self, training_step):\n",
        "        self.start_index = training_step\n",
        "        self.num_samples = int(\n",
        "            math.ceil(float(len(self.bins) - self.start_index) / self.num_replicas)\n",
        "        )\n",
        "        self.total_size = self.num_samples * self.num_replicas\n",
        "\n",
        "\n",
        "def audio_with_sox(path, sample_rate, start_time, end_time):\n",
        "    \"\"\"\n",
        "    crop and resample the recording with sox and loads it.\n",
        "    \"\"\"\n",
        "    with NamedTemporaryFile(suffix=\".wav\") as tar_file:\n",
        "        tar_filename = tar_file.name\n",
        "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} trim {} ={} >/dev/null 2>&1\".format(path, sample_rate,\n",
        "                                                                                               tar_filename, start_time,\n",
        "                                                                                               end_time)\n",
        "        os.system(sox_params)\n",
        "        y = load_audio(tar_filename)\n",
        "        return y\n",
        "\n",
        "\n",
        "def augment_audio_with_sox(path, sample_rate, tempo, gain):\n",
        "    \"\"\"\n",
        "    Changes tempo and gain of the recording with sox and loads it.\n",
        "    \"\"\"\n",
        "    with NamedTemporaryFile(suffix=\".wav\") as augmented_file:\n",
        "        augmented_filename = augmented_file.name\n",
        "        sox_augment_params = [\"tempo\", \"{:.3f}\".format(tempo), \"gain\", \"{:.3f}\".format(gain)]\n",
        "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} {} >/dev/null 2>&1\".format(path, sample_rate,\n",
        "                                                                                      augmented_filename,\n",
        "                                                                                      \" \".join(sox_augment_params))\n",
        "        os.system(sox_params)\n",
        "        y = load_audio(augmented_filename)\n",
        "        return y\n",
        "\n",
        "\n",
        "def load_randomly_augmented_audio(path, sample_rate=16000, tempo_range=(0.85, 1.15),\n",
        "                                  gain_range=(-6, 8)):\n",
        "    \"\"\"\n",
        "    Picks tempo and gain uniformly, applies it to the utterance by using sox utility.\n",
        "    Returns the augmented utterance.\n",
        "    \"\"\"\n",
        "    low_tempo, high_tempo = tempo_range\n",
        "    tempo_value = np.random.uniform(low=low_tempo, high=high_tempo)\n",
        "    low_gain, high_gain = gain_range\n",
        "    gain_value = np.random.uniform(low=low_gain, high=high_gain)\n",
        "    audio = augment_audio_with_sox(path=path, sample_rate=sample_rate,\n",
        "                                   tempo=tempo_value, gain=gain_value)\n",
        "    return audio"
      ],
      "metadata": {
        "id": "ihG2QnDPUa6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#opts\n"
      ],
      "metadata": {
        "id": "bEB16cMQWXoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_decoder_args(parser):\n",
        "    beam_args = parser.add_argument_group(\"Beam Decode Options\",\n",
        "                                          \"Configurations options for the CTC Beam Search decoder\")\n",
        "    beam_args.add_argument('--top-paths', default=1, type=int, help='number of beams to return')\n",
        "    beam_args.add_argument('--beam-width', default=10, type=int, help='Beam width to use')\n",
        "    beam_args.add_argument('--lm-path', default=None, type=str,\n",
        "                           help='Path to an (optional) kenlm language model for use with beam search (req\\'d with trie)')\n",
        "    beam_args.add_argument('--alpha', default=0.8, type=float, help='Language model weight')\n",
        "    beam_args.add_argument('--beta', default=1, type=float, help='Language model word bonus (all words)')\n",
        "    beam_args.add_argument('--cutoff-top-n', default=40, type=int,\n",
        "                           help='Cutoff number in pruning, only top cutoff_top_n characters with highest probs in '\n",
        "                                'vocabulary will be used in beam search, default 40.')\n",
        "    beam_args.add_argument('--cutoff-prob', default=1.0, type=float,\n",
        "                           help='Cutoff probability in pruning,default 1.0, no pruning.')\n",
        "    beam_args.add_argument('--lm-workers', default=1, type=int, help='Number of LM processes to use')\n",
        "    return parser\n",
        "\n",
        "\n",
        "def add_inference_args(parser):\n",
        "    parser.add_argument('--cuda', action=\"store_true\", help='Use cuda')\n",
        "    parser.add_argument('--half', action=\"store_true\",\n",
        "                        help='Use half precision. This is recommended when using mixed-precision at training time')\n",
        "    parser.add_argument('--decoder', default=\"greedy\", choices=[\"greedy\", \"beam\"], type=str, help=\"Decoder to use\")\n",
        "    parser.add_argument('--model-path', default='models/deepspeech_final.pth',\n",
        "                        help='Path to model file created by training')\n",
        "    return parser"
      ],
      "metadata": {
        "id": "e56YSZm0WZEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#logger"
      ],
      "metadata": {
        "id": "f2u5HDWsWbe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_decoder_args(parser):\n",
        "    beam_args = parser.add_argument_group(\"Beam Decode Options\",\n",
        "                                          \"Configurations options for the CTC Beam Search decoder\")\n",
        "    beam_args.add_argument('--top-paths', default=1, type=int, help='number of beams to return')\n",
        "    beam_args.add_argument('--beam-width', default=10, type=int, help='Beam width to use')\n",
        "    beam_args.add_argument('--lm-path', default=None, type=str,\n",
        "                           help='Path to an (optional) kenlm language model for use with beam search (req\\'d with trie)')\n",
        "    beam_args.add_argument('--alpha', default=0.8, type=float, help='Language model weight')\n",
        "    beam_args.add_argument('--beta', default=1, type=float, help='Language model word bonus (all words)')\n",
        "    beam_args.add_argument('--cutoff-top-n', default=40, type=int,\n",
        "                           help='Cutoff number in pruning, only top cutoff_top_n characters with highest probs in '\n",
        "                                'vocabulary will be used in beam search, default 40.')\n",
        "    beam_args.add_argument('--cutoff-prob', default=1.0, type=float,\n",
        "                           help='Cutoff probability in pruning,default 1.0, no pruning.')\n",
        "    beam_args.add_argument('--lm-workers', default=1, type=int, help='Number of LM processes to use')\n",
        "    return parser\n",
        "\n",
        "\n",
        "def add_inference_args(parser):\n",
        "    parser.add_argument('--cuda', action=\"store_true\", help='Use cuda')\n",
        "    parser.add_argument('--half', action=\"store_true\",\n",
        "                        help='Use half precision. This is recommended when using mixed-precision at training time')\n",
        "    parser.add_argument('--decoder', default=\"greedy\", choices=[\"greedy\", \"beam\"], type=str, help=\"Decoder to use\")\n",
        "    parser.add_argument('--model-path', default='models/deepspeech_final.pth',\n",
        "                        help='Path to model file created by training')\n",
        "    return parser"
      ],
      "metadata": {
        "id": "OVMGfdVqWdc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#logger"
      ],
      "metadata": {
        "id": "F2r24bPJWfdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def to_np(x):\n",
        "    return x.cpu().numpy()\n",
        "\n",
        "\n",
        "class VisdomLogger(object):\n",
        "    def __init__(self, id, num_epochs):\n",
        "        from visdom import Visdom\n",
        "        self.viz = Visdom()\n",
        "        self.opts = dict(title=id, ylabel='', xlabel='Epoch', legend=['Loss', 'WER', 'CER'])\n",
        "        self.viz_window = None\n",
        "        self.epochs = torch.arange(1, num_epochs + 1)\n",
        "        self.visdom_plotter = True\n",
        "\n",
        "    def update(self, epoch, values):\n",
        "        x_axis = self.epochs[0:epoch + 1]\n",
        "        y_axis = torch.stack((values.loss_results[:epoch],\n",
        "                              values.wer_results[:epoch],\n",
        "                              values.cer_results[:epoch]),\n",
        "                             dim=1)\n",
        "        self.viz_window = self.viz.line(\n",
        "            X=x_axis,\n",
        "            Y=y_axis,\n",
        "            opts=self.opts,\n",
        "            win=self.viz_window,\n",
        "            update='replace' if self.viz_window else None\n",
        "        )\n",
        "\n",
        "    def load_previous_values(self, start_epoch, results_state):\n",
        "        self.update(start_epoch - 1, results_state)  # Add all values except the iteration we're starting from\n",
        "\n",
        "\n",
        "class TensorBoardLogger(object):\n",
        "    def __init__(self, id, log_dir, log_params):\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        from torch.utils.tensorboard import SummaryWriter\n",
        "        self.id = id\n",
        "        self.tensorboard_writer = SummaryWriter(log_dir)\n",
        "        self.log_params = log_params\n",
        "\n",
        "    def update(self, epoch, results_state, parameters=None):\n",
        "        loss = results_state.loss_results[epoch]\n",
        "        wer = results_state.wer_results[epoch]\n",
        "        cer = results_state.cer_results[epoch]\n",
        "        values = {\n",
        "            'Avg Train Loss': loss,\n",
        "            'Avg WER': wer,\n",
        "            'Avg CER': cer\n",
        "        }\n",
        "        self.tensorboard_writer.add_scalars(self.id, values, epoch + 1)\n",
        "        if self.log_params:\n",
        "            for tag, value in parameters():\n",
        "                tag = tag.replace('.', '/')\n",
        "                self.tensorboard_writer.add_histogram(tag, to_np(value), epoch + 1)\n",
        "                self.tensorboard_writer.add_histogram(tag + '/grad', to_np(value.grad), epoch + 1)\n",
        "\n",
        "    def load_previous_values(self, start_epoch, result_state):\n",
        "        loss_results = result_state.loss_results[:start_epoch]\n",
        "        wer_results = result_state.wer_results[:start_epoch]\n",
        "        cer_results = result_state.cer_results[:start_epoch]\n",
        "\n",
        "        for i in range(start_epoch):\n",
        "            values = {\n",
        "                'Avg Train Loss': loss_results[i],\n",
        "                'Avg WER': wer_results[i],\n",
        "                'Avg CER': cer_results[i]\n",
        "            }\n",
        "            self.tensorboard_writer.add_scalars(self.id, values, i + 1)"
      ],
      "metadata": {
        "id": "ROJL_N43WfCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#decoder"
      ],
      "metadata": {
        "id": "XfjfZzQoWi2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# ----------------------------------------------------------------------------\n",
        "# Copyright 2015-2016 Nervana Systems Inc.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ----------------------------------------------------------------------------\n",
        "# Modified to support pytorch Tensors\n",
        "\n",
        "import Levenshtein as Lev\n",
        "import torch\n",
        "from six.moves import xrange\n",
        "\n",
        "\n",
        "class Decoder(object):\n",
        "    \"\"\"\n",
        "    Basic decoder class from which all other decoders inherit. Implements several\n",
        "    helper functions. Subclasses should implement the decode() method.\n",
        "\n",
        "    Arguments:\n",
        "        labels (list): mapping from integers to characters.\n",
        "        blank_index (int, optional): index for the blank '_' character. Defaults to 0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, blank_index=0):\n",
        "        self.labels = labels\n",
        "        self.int_to_char = dict([(i, c) for (i, c) in enumerate(labels)])\n",
        "        self.blank_index = blank_index\n",
        "        space_index = len(labels)  # To prevent errors in decode, we add an out of bounds index for the space\n",
        "        if ' ' in labels:\n",
        "            space_index = labels.index(' ')\n",
        "        self.space_index = space_index\n",
        "\n",
        "    def wer(self, s1, s2):\n",
        "        \"\"\"\n",
        "        Computes the Word Error Rate, defined as the edit distance between the\n",
        "        two provided sentences after tokenizing to words.\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "\n",
        "        # build mapping of words to integers\n",
        "        b = set(s1.split() + s2.split())\n",
        "        word2char = dict(zip(b, range(len(b))))\n",
        "\n",
        "        # map the words to a char array (Levenshtein packages only accepts\n",
        "        # strings)\n",
        "        w1 = [chr(word2char[w]) for w in s1.split()]\n",
        "        w2 = [chr(word2char[w]) for w in s2.split()]\n",
        "\n",
        "        return Lev.distance(''.join(w1), ''.join(w2))\n",
        "\n",
        "    def cer(self, s1, s2):\n",
        "        \"\"\"\n",
        "        Computes the Character Error Rate, defined as the edit distance.\n",
        "\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "        s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
        "        return Lev.distance(s1, s2)\n",
        "\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Given a matrix of character probabilities, returns the decoder's\n",
        "        best guess of the transcription\n",
        "\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities, where probs[c,t]\n",
        "                            is the probability of character c at time t\n",
        "            sizes(optional): Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            string: sequence of the model's best guess for the transcription\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BeamCTCDecoder(Decoder):\n",
        "    def __init__(self,\n",
        "                 labels,\n",
        "                 lm_path=None,\n",
        "                 alpha=0,\n",
        "                 beta=0,\n",
        "                 cutoff_top_n=40,\n",
        "                 cutoff_prob=1.0,\n",
        "                 beam_width=100,\n",
        "                 num_processes=4,\n",
        "                 blank_index=0):\n",
        "        super(BeamCTCDecoder, self).__init__(labels)\n",
        "        try:\n",
        "            from ctcdecode import CTCBeamDecoder\n",
        "        except ImportError:\n",
        "            raise ImportError(\"BeamCTCDecoder requires paddledecoder package.\")\n",
        "        labels = list(labels)  # Ensure labels are a list before passing to decoder\n",
        "        self._decoder = CTCBeamDecoder(labels, lm_path, alpha, beta, cutoff_top_n, cutoff_prob, beam_width,\n",
        "                                       num_processes, blank_index)\n",
        "\n",
        "    def convert_to_strings(self, out, seq_len):\n",
        "        results = []\n",
        "        for b, batch in enumerate(out):\n",
        "            utterances = []\n",
        "            for p, utt in enumerate(batch):\n",
        "                size = seq_len[b][p]\n",
        "                if size > 0:\n",
        "                    transcript = ''.join(map(lambda x: self.int_to_char[x.item()], utt[0:size]))\n",
        "                else:\n",
        "                    transcript = ''\n",
        "                utterances.append(transcript)\n",
        "            results.append(utterances)\n",
        "        return results\n",
        "\n",
        "    def convert_tensor(self, offsets, sizes):\n",
        "        results = []\n",
        "        for b, batch in enumerate(offsets):\n",
        "            utterances = []\n",
        "            for p, utt in enumerate(batch):\n",
        "                size = sizes[b][p]\n",
        "                if sizes[b][p] > 0:\n",
        "                    utterances.append(utt[0:size])\n",
        "                else:\n",
        "                    utterances.append(torch.tensor([], dtype=torch.int))\n",
        "            results.append(utterances)\n",
        "        return results\n",
        "\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Decodes probability output using ctcdecode package.\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities, where probs[c,t]\n",
        "                            is the probability of character c at time t\n",
        "            sizes: Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            string: sequences of the model's best guess for the transcription\n",
        "        \"\"\"\n",
        "        probs = probs.cpu()\n",
        "        out, scores, offsets, seq_lens = self._decoder.decode(probs, sizes)\n",
        "\n",
        "        strings = self.convert_to_strings(out, seq_lens)\n",
        "        offsets = self.convert_tensor(offsets, seq_lens)\n",
        "        return strings, offsets\n",
        "\n",
        "\n",
        "class GreedyDecoder(Decoder):\n",
        "    def __init__(self, labels, blank_index=0):\n",
        "        super(GreedyDecoder, self).__init__(labels, blank_index)\n",
        "\n",
        "    def convert_to_strings(self,\n",
        "                           sequences,\n",
        "                           sizes=None,\n",
        "                           remove_repetitions=False,\n",
        "                           return_offsets=False):\n",
        "        \"\"\"Given a list of numeric sequences, returns the corresponding strings\"\"\"\n",
        "        strings = []\n",
        "        offsets = [] if return_offsets else None\n",
        "        for x in xrange(len(sequences)):\n",
        "            seq_len = sizes[x] if sizes is not None else len(sequences[x])\n",
        "            string, string_offsets = self.process_string(sequences[x], seq_len, remove_repetitions)\n",
        "            strings.append([string])  # We only return one path\n",
        "            if return_offsets:\n",
        "                offsets.append([string_offsets])\n",
        "        if return_offsets:\n",
        "            return strings, offsets\n",
        "        else:\n",
        "            return strings\n",
        "\n",
        "    def process_string(self,\n",
        "                       sequence,\n",
        "                       size,\n",
        "                       remove_repetitions=False):\n",
        "        string = ''\n",
        "        offsets = []\n",
        "        for i in range(size):\n",
        "            char = self.int_to_char[sequence[i].item()]\n",
        "            if char != self.int_to_char[self.blank_index]:\n",
        "                # if this char is a repetition and remove_repetitions=true, then skip\n",
        "                if remove_repetitions and i != 0 and char == self.int_to_char[sequence[i - 1].item()]:\n",
        "                    pass\n",
        "                elif char == self.labels[self.space_index]:\n",
        "                    string += ' '\n",
        "                    offsets.append(i)\n",
        "                else:\n",
        "                    string = string + char\n",
        "                    offsets.append(i)\n",
        "        return string, torch.tensor(offsets, dtype=torch.int)\n",
        "\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Returns the argmax decoding given the probability matrix. Removes\n",
        "        repeated elements in the sequence, as well as blanks.\n",
        "\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities from the network. Expected shape of batch x seq_length x output_dim\n",
        "            sizes(optional): Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            strings: sequences of the model's best guess for the transcription on inputs\n",
        "            offsets: time step per character predicted\n",
        "        \"\"\"\n",
        "        _, max_probs = torch.max(probs, 2)\n",
        "        strings, offsets = self.convert_to_strings(max_probs.view(max_probs.size(0), max_probs.size(1)),\n",
        "                                                   sizes,\n",
        "                                                   remove_repetitions=True,\n",
        "                                                   return_offsets=True)\n",
        "        return strings, offsets"
      ],
      "metadata": {
        "id": "qu2-winQWiiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model"
      ],
      "metadata": {
        "id": "IA-0g30bWnOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "supported_rnns = {\n",
        "    'lstm': nn.LSTM,\n",
        "    'rnn': nn.RNN,\n",
        "    'gru': nn.GRU\n",
        "}\n",
        "supported_rnns_inv = dict((v, k) for k, v in supported_rnns.items())\n",
        "\n",
        "#batch normalization\n",
        "class SequenceWise(nn.Module):\n",
        "    def __init__(self, module):\n",
        "        \"\"\"\n",
        "        Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
        "        Allows handling of variable sequence lengths and minibatch sizes.\n",
        "        :param module: Module to apply input to.\n",
        "        \"\"\"\n",
        "        super(SequenceWise, self).__init__()\n",
        "        self.module = module # batchnorm1d(num_features, eps,~~~)\n",
        "\n",
        "    def forward(self, x):\n",
        "        t, n = x.size(0), x.size(1)\n",
        "        x = x.view(t * n, -1)\n",
        "        x = self.module(x)\n",
        "        x = x.view(t, n, -1)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        '''\n",
        "        __repr__ should return a printable representation of the object,\n",
        "         most likely one of the ways possible to create this object.\n",
        "        example:\n",
        "        >>> class Point:\n",
        "        ...   def __init__(self, x, y):\n",
        "        ...     self.x, self.y = x, y\n",
        "        ...   def __repr__(self):\n",
        "        ...     return 'Point(x=%s, y=%s)' % (self.x, self.y)\n",
        "        >>> p = Point(1, 2)\n",
        "        >>> p\n",
        "        Point(x=1, y=2)\n",
        "        '''\n",
        "        tmpstr = self.__class__.__name__ + ' (\\n'\n",
        "        tmpstr += self.module.__repr__()\n",
        "        tmpstr += ')'\n",
        "        return tmpstr\n",
        "\n",
        "\n",
        "class MaskConv(nn.Module):\n",
        "    def __init__(self, seq_module):\n",
        "        \"\"\"\n",
        "        Adds padding to the output of the module based on the given lengths. This is to ensure that the\n",
        "        results of the model do not change when batch sizes change during inference.\n",
        "        Input needs to be in the shape of (BxCxDxT)\n",
        "        :param seq_module: The sequential module containing the conv stack.\n",
        "        \"\"\"\n",
        "        super(MaskConv, self).__init__()\n",
        "        self.seq_module = seq_module\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"\n",
        "        :param x: The input of size BxCxDxT\n",
        "        :param lengths: The actual length of each sequence in the batch\n",
        "        :return: Masked output from the module\n",
        "        \"\"\"\n",
        "        for module in self.seq_module:\n",
        "            x = module(x)\n",
        "            mask = torch.BoolTensor(x.size()).fill_(0)\n",
        "            if x.is_cuda:\n",
        "                mask = mask.cuda()\n",
        "            for i, length in enumerate(lengths):\n",
        "                length = length.item()\n",
        "                if (mask[i].size(2) - length) > 0:\n",
        "                    mask[i].narrow(2, length, mask[i].size(2) - length).fill_(1)\n",
        "            x = x.masked_fill(mask, 0)\n",
        "        return x, lengths\n",
        "\n",
        "\n",
        "class InferenceBatchSoftmax(nn.Module):\n",
        "    def forward(self, input_):\n",
        "        if not self.training:\n",
        "            return F.softmax(input_, dim=-1)\n",
        "        else:\n",
        "            return input_\n",
        "\n",
        "\n",
        "class BatchRNN(nn.Module):\n",
        "    '''\n",
        "    RNNcell -> through time backpropagation\n",
        "    RNN -> one pass of backpropagation\n",
        "\n",
        "    LSTMCell takes ONE input x_t. You need to make a loop in order to do one pass of backprop through time.\n",
        "    LSTM takes a SEQUENCE of inputs x_1,x_2,…,x_T. No need to write a loop to do one pass of backprop through time.\n",
        "    https://discuss.pytorch.org/t/lstm-and-lstmcell/7488\n",
        "    '''\n",
        "    def __init__(self, input_size, hidden_size, rnn_type=nn.LSTM, bidirectional=False, batch_norm=True):\n",
        "        super(BatchRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)) if batch_norm else None\n",
        "        self.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,\n",
        "                            bidirectional=bidirectional, bias=True)\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        self.rnn.flatten_parameters()\n",
        "\n",
        "    def forward(self, x, output_lengths):\n",
        "        if self.batch_norm is not None:\n",
        "            x = self.batch_norm(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, output_lengths)\n",
        "        #PackedSequence object 를 얻는다.\n",
        "        x, h = self.rnn(x)\n",
        "        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n",
        "        #pack된걸 unpack한다.\n",
        "\n",
        "        if self.bidirectional:\n",
        "            x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1)  # (TxNxH*2) -> (TxNxH) by sum\n",
        "        return x\n",
        "\n",
        "\n",
        "class Lookahead(nn.Module):\n",
        "    # Wang et al 2016 - Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks\n",
        "    # input shape - sequence, batch, feature - TxNxH\n",
        "    # output shape - same as input\n",
        "    def __init__(self, n_features, context):\n",
        "        super(Lookahead, self).__init__()\n",
        "        assert context > 0\n",
        "        self.context = context\n",
        "        self.n_features = n_features\n",
        "        self.pad = (0, self.context - 1)\n",
        "        self.conv = nn.Conv1d(self.n_features, self.n_features, kernel_size=self.context, stride=1,\n",
        "                              groups=self.n_features, padding=0, bias=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(0, 1).transpose(1, 2)\n",
        "        x = F.pad(x, pad=self.pad, value=0)\n",
        "        x = self.conv(x)\n",
        "        x = x.transpose(1, 2).transpose(0, 1).contiguous()\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + 'n_features=' + str(self.n_features) \\\n",
        "               + ', context=' + str(self.context) + ')'\n",
        "\n",
        "\n",
        "class DeepSpeech(nn.Module):\n",
        "    def __init__(self, rnn_type, labels, rnn_hidden_size, nb_layers, audio_conf,\n",
        "                 bidirectional, context=20):\n",
        "        super(DeepSpeech, self).__init__()\n",
        "\n",
        "        self.hidden_size = rnn_hidden_size\n",
        "        self.hidden_layers = nb_layers\n",
        "        self.rnn_type = rnn_type\n",
        "        self.audio_conf = audio_conf\n",
        "        self.labels = labels\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        sample_rate = self.audio_conf[\"sample_rate\"]\n",
        "        window_size = self.audio_conf[\"window_size\"]\n",
        "        num_classes = len(self.labels)\n",
        "\n",
        "        # conv layer\n",
        "        self.conv = MaskConv(nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Hardtanh(0, 20, inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Hardtanh(0, 20, inplace=True)\n",
        "        ))\n",
        "        # Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1\n",
        "        # 위에서 convlayer를 통과했으므로 패딩과 스트라이드에 따라서 rnn_input_size를 조절해줘야 한다.\n",
        "        rnn_input_size = int(math.floor((sample_rate * window_size) / 2) + 1)\n",
        "        rnn_input_size = int(math.floor(rnn_input_size + 2 * 20 - 41) / 2 + 1)\n",
        "        rnn_input_size = int(math.floor(rnn_input_size + 2 * 10 - 21) / 2 + 1)\n",
        "        rnn_input_size *= 32\n",
        "\n",
        "        # RNN layer\n",
        "        rnns = []\n",
        "\n",
        "        # 첫번재 RNNlayer는 batch_norm 하지 않는다. 왜??\n",
        "        rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
        "                       bidirectional=bidirectional, batch_norm=False)\n",
        "        rnns.append(('0', rnn))\n",
        "\n",
        "        for x in range(nb_layers - 1):\n",
        "            rnn = BatchRNN(input_size=rnn_hidden_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
        "                           bidirectional=bidirectional)\n",
        "            rnns.append(('%d' % (x + 1), rnn))\n",
        "\n",
        "        # OrderedDict - 이름과 레이어를 같이 만듬\n",
        "        self.rnns = nn.Sequential(OrderedDict(rnns))\n",
        "        self.lookahead = nn.Sequential(\n",
        "            # consider adding batch norm?\n",
        "            Lookahead(rnn_hidden_size, context=context),\n",
        "            nn.Hardtanh(0, 20, inplace=True)\n",
        "        ) if not bidirectional else None\n",
        "\n",
        "        fully_connected = nn.Sequential(\n",
        "            nn.BatchNorm1d(rnn_hidden_size),\n",
        "            nn.Linear(rnn_hidden_size, num_classes, bias=False)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            SequenceWise(fully_connected),\n",
        "        )\n",
        "        self.inference_softmax = InferenceBatchSoftmax()\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        lengths = lengths.cpu().int()\n",
        "        output_lengths = self.get_seq_lens(lengths)\n",
        "\n",
        "        # conv layer\n",
        "        x, _ = self.conv(x, output_lengths)\n",
        "\n",
        "        sizes = x.size()\n",
        "\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
        "        x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
        "\n",
        "        for rnn in self.rnns:\n",
        "            x = rnn(x, output_lengths) # ex) 268 x 20 x 1024\n",
        "\n",
        "        if not self.bidirectional:  # no need for lookahead layer in bidirectional\n",
        "            x = self.lookahead(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        # identity in training mode, softmax in eval mode\n",
        "        x = self.inference_softmax(x)\n",
        "        return x, output_lengths\n",
        "\n",
        "    def get_seq_lens(self, input_length):\n",
        "        \"\"\"\n",
        "        Given a 1D Tensor or Variable containing integer sequence lengths, return a 1D tensor or variable\n",
        "        containing the size sequences that will be output by the network.\n",
        "        :param input_length: 1D Tensor\n",
        "        :return: 1D Tensor scaled by model\n",
        "        \"\"\"\n",
        "        seq_len = input_length\n",
        "        for m in self.conv.modules():\n",
        "            if type(m) == nn.modules.conv.Conv2d:\n",
        "                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n",
        "        return seq_len.int()\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, path):\n",
        "        package = torch.load(path, map_location=lambda storage, loc: storage)\n",
        "        model = DeepSpeech.load_model_package(package)\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def load_model_package(cls, package):\n",
        "        model = cls(rnn_hidden_size=package['hidden_size'],\n",
        "                    nb_layers=package['hidden_layers'],\n",
        "                    labels=package['labels'],\n",
        "                    audio_conf=package['audio_conf'],\n",
        "                    rnn_type=supported_rnns[package['rnn_type']],\n",
        "                    bidirectional=package.get('bidirectional', True))\n",
        "        model.load_state_dict(package['state_dict'])\n",
        "        return model\n",
        "\n",
        "    def serialize_state(self):\n",
        "        return {\n",
        "            'hidden_size': self.hidden_size,\n",
        "            'hidden_layers': self.hidden_layers,\n",
        "            'rnn_type': supported_rnns_inv.get(self.rnn_type, self.rnn_type.__name__.lower()),\n",
        "            'audio_conf': self.audio_conf,\n",
        "            'labels': self.labels,\n",
        "            'state_dict': self.state_dict(),\n",
        "            'bidirectional': self.bidirectional,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def get_param_size(model):\n",
        "        params = 0\n",
        "        for p in model.parameters():\n",
        "            tmp = 1\n",
        "            for x in p.size():\n",
        "                tmp *= x\n",
        "            params += tmp\n",
        "        return params"
      ],
      "metadata": {
        "id": "2yL6dbIAWmGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils"
      ],
      "metadata": {
        "id": "2DxlL5qwWpXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from deepspeech_pytorch.decoder import GreedyDecoder\n",
        "from deepspeech_pytorch.model import DeepSpeech\n",
        "\n",
        "\n",
        "class CheckpointHandler:\n",
        "    def __init__(self,\n",
        "                 save_folder: str,\n",
        "                 best_val_model_name: str,\n",
        "                 checkpoint_per_iteration: int,\n",
        "                 save_n_recent_models: int):\n",
        "        self.save_folder = Path(save_folder)\n",
        "        self.save_folder.mkdir(parents=True, exist_ok=True)  # Ensure save folder exists\n",
        "        self.checkpoint_prefix = 'deepspeech_checkpoint_'  # TODO do we want to expose this?\n",
        "        self.checkpoint_prefix_path = self.save_folder / self.checkpoint_prefix\n",
        "        self.best_val_path = self.save_folder / best_val_model_name\n",
        "        self.checkpoint_per_iteration = checkpoint_per_iteration\n",
        "        self.save_n_recent_models = save_n_recent_models\n",
        "\n",
        "    def find_latest_checkpoint(self):\n",
        "        \"\"\"\n",
        "        Finds the latest checkpoint in a folder based on the timestamp of the file.\n",
        "        If there are no checkpoints, returns None.\n",
        "        :return: The latest checkpoint path, or None if no checkpoints are found.\n",
        "        \"\"\"\n",
        "        paths = list(self.save_folder.rglob(self.checkpoint_prefix + '*'))\n",
        "        if paths:\n",
        "            paths.sort(key=os.path.getctime)\n",
        "            latest_checkpoint_path = paths[-1]\n",
        "            return latest_checkpoint_path\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def check_and_delete_oldest_checkpoint(self):\n",
        "        paths = list(self.save_folder.rglob(self.checkpoint_prefix + '*'))\n",
        "        if paths and len(paths) >= self.save_n_recent_models:\n",
        "            paths.sort(key=os.path.getctime)\n",
        "            print(\"Deleting old checkpoint %s\" % str(paths[0]))\n",
        "            os.remove(paths[0])\n",
        "\n",
        "    def save_checkpoint_model(self, epoch, state, i=None):\n",
        "        if self.save_n_recent_models > 0:\n",
        "            self.check_and_delete_oldest_checkpoint()\n",
        "        model_path = self._create_checkpoint_path(epoch=epoch,\n",
        "                                                  i=i)\n",
        "        print(\"Saving checkpoint model to %s\" % model_path)\n",
        "        torch.save(obj=state.serialize_state(epoch=epoch,\n",
        "                                             iteration=i),\n",
        "                   f=model_path)\n",
        "\n",
        "    def save_iter_checkpoint_model(self, epoch, state, i):\n",
        "        if self.checkpoint_per_iteration > 0 and i > 0 and (i + 1) % self.checkpoint_per_iteration == 0:\n",
        "            self.save_checkpoint_model(epoch=epoch,\n",
        "                                       state=state,\n",
        "                                       i=i)\n",
        "\n",
        "    def save_best_model(self, epoch, state):\n",
        "        print(\"Found better validated model, saving to %s\" % self.best_val_path)\n",
        "        torch.save(obj=state.serialize_state(epoch=epoch,\n",
        "                                             iteration=None),\n",
        "                   f=self.best_val_path)\n",
        "\n",
        "    def _create_checkpoint_path(self, epoch, i=None):\n",
        "        \"\"\"\n",
        "        Creates path to save checkpoint.\n",
        "        We automatically iterate the epoch and iteration for readibility.\n",
        "        :param epoch: The epoch (index starts at 0).\n",
        "        :param i: The iteration (index starts at 0).\n",
        "        :return: The path to save the model\n",
        "        \"\"\"\n",
        "        if i:\n",
        "            checkpoint_path = str(self.checkpoint_prefix_path) + 'epoch_%d_iter_%d.pth' % (epoch + 1, i + 1)\n",
        "        else:\n",
        "            checkpoint_path = str(self.checkpoint_prefix_path) + 'epoch_%d.pth' % (epoch + 1)\n",
        "        return checkpoint_path\n",
        "\n",
        "\n",
        "def check_loss(loss, loss_value):\n",
        "    \"\"\"\n",
        "    Check that warp-ctc loss is valid and will not break training\n",
        "    :return: Return if loss is valid, and the error in case it is not\n",
        "    \"\"\"\n",
        "    loss_valid = True\n",
        "    error = ''\n",
        "    if loss_value == float(\"inf\") or loss_value == float(\"-inf\"):\n",
        "        loss_valid = False\n",
        "        error = \"WARNING: received an inf loss\"\n",
        "    elif torch.isnan(loss).sum() > 0:\n",
        "        loss_valid = False\n",
        "        error = 'WARNING: received a nan loss, setting loss value to 0'\n",
        "    elif loss_value < 0:\n",
        "        loss_valid = False\n",
        "        error = \"WARNING: received a negative loss\"\n",
        "    return loss_valid, error\n",
        "\n",
        "\n",
        "def load_model(device,\n",
        "               model_path,\n",
        "               use_half):\n",
        "    model = DeepSpeech.load_model(model_path)\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    if use_half:\n",
        "        model = model.half()\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_decoder(decoder_type,\n",
        "                 labels,\n",
        "                 lm_path,\n",
        "                 alpha,\n",
        "                 beta,\n",
        "                 cutoff_top_n,\n",
        "                 cutoff_prob,\n",
        "                 beam_width,\n",
        "                 lm_workers):\n",
        "    if decoder_type == \"beam\":\n",
        "        from deepspeech_pytorch.decoder import BeamCTCDecoder\n",
        "\n",
        "        decoder = BeamCTCDecoder(labels=labels,\n",
        "                                 lm_path=lm_path,\n",
        "                                 alpha=alpha,\n",
        "                                 beta=beta,\n",
        "                                 cutoff_top_n=cutoff_top_n,\n",
        "                                 cutoff_prob=cutoff_prob,\n",
        "                                 beam_width=beam_width,\n",
        "                                 num_processes=lm_workers)\n",
        "    else:\n",
        "        decoder = GreedyDecoder(labels=labels,\n",
        "                                blank_index=labels.index('_'))\n",
        "    return decoder\n",
        "\n",
        "\n",
        "def remove_parallel_wrapper(model):\n",
        "    \"\"\"\n",
        "    Return the model or extract the model out of the parallel wrapper\n",
        "    :param model: The training model\n",
        "    :return: The model without parallel wrapper\n",
        "    \"\"\"\n",
        "    # Take care of distributed/data-parallel wrapper\n",
        "    model_no_wrapper = model.module if hasattr(model, \"module\") else model\n",
        "    return model_no_wrapper"
      ],
      "metadata": {
        "id": "_z0ds96-Ww_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#state"
      ],
      "metadata": {
        "id": "UOe-PEILWxxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from deepspeech_pytorch.model import DeepSpeech\n",
        "from deepspeech_pytorch.utils import remove_parallel_wrapper\n",
        "\n",
        "\n",
        "class ResultState:\n",
        "    def __init__(self,\n",
        "                 loss_results,\n",
        "                 wer_results,\n",
        "                 cer_results):\n",
        "        self.loss_results = loss_results\n",
        "        self.wer_results = wer_results\n",
        "        self.cer_results = cer_results\n",
        "\n",
        "    def add_results(self,\n",
        "                    epoch,\n",
        "                    loss_result,\n",
        "                    wer_result,\n",
        "                    cer_result):\n",
        "        self.loss_results[epoch] = loss_result\n",
        "        self.wer_results[epoch] = wer_result\n",
        "        self.cer_results[epoch] = cer_result\n",
        "\n",
        "    def serialize_state(self):\n",
        "        return {\n",
        "            'loss_results': self.loss_results,\n",
        "            'wer_results': self.wer_results,\n",
        "            'cer_results': self.cer_results\n",
        "        }\n",
        "\n",
        "\n",
        "class TrainingState:\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 result_state=None,\n",
        "                 optim_state=None,\n",
        "                 amp_state=None,\n",
        "                 best_wer=None,\n",
        "                 avg_loss=0,\n",
        "                 epoch=0,\n",
        "                 training_step=0):\n",
        "        \"\"\"\n",
        "        Wraps around training model and states for saving/loading convenience.\n",
        "        For backwards compatibility there are more states being saved than necessary.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.result_state = result_state\n",
        "        self.optim_state = optim_state\n",
        "        self.amp_state = amp_state\n",
        "        self.best_wer = best_wer\n",
        "        self.avg_loss = avg_loss\n",
        "        self.epoch = epoch\n",
        "        self.training_step = training_step\n",
        "\n",
        "    def track_optim_state(self, optimizer):\n",
        "        self.optim_state = optimizer.state_dict()\n",
        "\n",
        "    def track_amp_state(self, amp):\n",
        "        self.amp_state = amp.state_dict()\n",
        "\n",
        "    def init_results_tracking(self, epochs):\n",
        "        self.result_state = ResultState(loss_results=torch.IntTensor(epochs),\n",
        "                                        wer_results=torch.IntTensor(epochs),\n",
        "                                        cer_results=torch.IntTensor(epochs))\n",
        "\n",
        "    def add_results(self,\n",
        "                    epoch,\n",
        "                    loss_result,\n",
        "                    wer_result,\n",
        "                    cer_result):\n",
        "        self.result_state.add_results(epoch=epoch,\n",
        "                                      loss_result=loss_result,\n",
        "                                      wer_result=wer_result,\n",
        "                                      cer_result=cer_result)\n",
        "\n",
        "    def init_finetune_states(self, epochs):\n",
        "        \"\"\"\n",
        "        Resets the training environment, but keeps model specific states in tact.\n",
        "        This is when fine-tuning a model on another dataset where training is to be reset but the model\n",
        "        weights are to be loaded\n",
        "        :param epochs: Number of epochs fine-tuning.\n",
        "        \"\"\"\n",
        "        self.init_results_tracking(epochs)\n",
        "        self._reset_amp_state()\n",
        "        self._reset_optim_state()\n",
        "        self._reset_epoch()\n",
        "        self.reset_training_step()\n",
        "        self._reset_best_wer()\n",
        "\n",
        "    def serialize_state(self, epoch, iteration):\n",
        "        model = remove_parallel_wrapper(self.model)\n",
        "        model_dict = model.serialize_state()\n",
        "        training_dict = self._serialize_training_state(epoch=epoch,\n",
        "                                                       iteration=iteration)\n",
        "        results_dict = self.result_state.serialize_state()\n",
        "        # Ensure flat structure for backwards compatibility\n",
        "        state_dict = {**model_dict, **training_dict, **results_dict}  # Combine dicts\n",
        "        return state_dict\n",
        "\n",
        "    def _serialize_training_state(self, epoch, iteration):\n",
        "        return {\n",
        "            'optim_dict': self.optim_state,\n",
        "            'amp': self.amp_state,\n",
        "            'avg_loss': self.avg_loss,\n",
        "            'best_wer': self.best_wer,\n",
        "            'epoch': epoch + 1,  # increment for readability\n",
        "            'iteration': iteration,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def load_state(cls, state_path):\n",
        "        print(\"Loading state from model %s\" % state_path)\n",
        "        state = torch.load(state_path, map_location=lambda storage, loc: storage)\n",
        "        model = DeepSpeech.load_model_package(state)\n",
        "        optim_state = state['optim_dict']\n",
        "        amp_state = state['amp']\n",
        "        epoch = int(state.get('epoch', 1)) - 1  # Index start at 0 for training\n",
        "        training_step = state.get('iteration', None)\n",
        "        if training_step is None:\n",
        "            epoch += 1  # We saved model after epoch finished, start at the next epoch.\n",
        "            training_step = 0\n",
        "        else:\n",
        "            training_step += 1\n",
        "        avg_loss = int(state.get('avg_loss', 0))\n",
        "        loss_results = state['loss_results']\n",
        "        cer_results = state['cer_results']\n",
        "        wer_results = state['wer_results']\n",
        "        best_wer = state.get('best_wer')\n",
        "\n",
        "        result_state = ResultState(loss_results=loss_results,\n",
        "                                   cer_results=cer_results,\n",
        "                                   wer_results=wer_results)\n",
        "        return cls(optim_state=optim_state,\n",
        "                   amp_state=amp_state,\n",
        "                   model=model,\n",
        "                   result_state=result_state,\n",
        "                   best_wer=best_wer,\n",
        "                   avg_loss=avg_loss,\n",
        "                   epoch=epoch,\n",
        "                   training_step=training_step)\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        self.epoch = epoch\n",
        "\n",
        "    def set_best_wer(self, wer):\n",
        "        self.best_wer = wer\n",
        "\n",
        "    def set_training_step(self, training_step):\n",
        "        self.training_step = training_step\n",
        "\n",
        "    def reset_training_step(self):\n",
        "        self.training_step = 0\n",
        "\n",
        "    def reset_avg_loss(self):\n",
        "        self.avg_loss = 0\n",
        "\n",
        "    def _reset_amp_state(self):\n",
        "        self.amp_state = None\n",
        "\n",
        "    def _reset_optim_state(self):\n",
        "        self.optim_state = None\n",
        "\n",
        "    def _reset_epoch(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def _reset_best_wer(self):\n",
        "        self.best_wer = None"
      ],
      "metadata": {
        "id": "IJcAOcIhW0HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# merge_manifests"
      ],
      "metadata": {
        "id": "CASU6Ah3UrYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import io\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "from utils import order_and_prune_files\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Merges all manifest CSV files in specified folder.')\n",
        "parser.add_argument('--merge-dir', default='manifests/', help='Path to all manifest files you want to merge')\n",
        "parser.add_argument('--min-duration', default=1, type=int,\n",
        "                    help='Prunes any samples shorter than the min duration (given in seconds, default 1)')\n",
        "parser.add_argument('--max-duration', default=15, type=int,\n",
        "                    help='Prunes any samples longer than the max duration (given in seconds, default 15)')\n",
        "parser.add_argument('--output-path', default='merged_manifest.csv', help='Output path to merged manifest')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "file_paths = []\n",
        "for file in os.listdir(args.merge_dir):\n",
        "    if file.endswith(\".csv\"):\n",
        "        with open(os.path.join(args.merge_dir, file), 'r') as fh:\n",
        "            file_paths += fh.readlines()\n",
        "file_paths = [file_path.split(',')[0] for file_path in file_paths]\n",
        "file_paths = order_and_prune_files(file_paths, args.min_duration, args.max_duration)\n",
        "with io.FileIO(args.output_path, \"w\") as file:\n",
        "    for wav_path in tqdm(file_paths, total=len(file_paths)):\n",
        "        transcript_path = wav_path.replace('/wav/', '/txt/').replace('.wav', '.txt')\n",
        "        sample = os.path.abspath(wav_path) + ',' + os.path.abspath(transcript_path) + '\\n'\n",
        "        file.write(sample.encode('utf-8'))"
      ],
      "metadata": {
        "id": "q3qjuIyVUrAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#testing"
      ],
      "metadata": {
        "id": "e6g0fpppW3I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def evaluate(test_loader,\n",
        "             device,\n",
        "             model,\n",
        "             decoder,\n",
        "             target_decoder,\n",
        "             save_output=None,\n",
        "             verbose=False,\n",
        "             half=False):\n",
        "    model.eval()\n",
        "    total_cer, total_wer, num_tokens, num_chars = 0, 0, 0, 0\n",
        "    output_data = []\n",
        "    for i, (data) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "        inputs, targets, input_percentages, target_sizes = data\n",
        "        input_sizes = input_percentages.mul_(int(inputs.size(3))).int()\n",
        "        inputs = inputs.to(device)\n",
        "        if half:\n",
        "            inputs = inputs.half()\n",
        "        # unflatten targets\n",
        "        split_targets = []\n",
        "        offset = 0\n",
        "        for size in target_sizes:\n",
        "            split_targets.append(targets[offset:offset + size])\n",
        "            offset += size\n",
        "\n",
        "        out, output_sizes = model(inputs, input_sizes)\n",
        "\n",
        "        decoded_output, _ = decoder.decode(out, output_sizes)\n",
        "        target_strings = target_decoder.convert_to_strings(split_targets)\n",
        "\n",
        "        if save_output is not None:\n",
        "            # add output to data array, and continue\n",
        "            output_data.append((out.cpu(), output_sizes, target_strings))\n",
        "        for x in range(len(target_strings)):\n",
        "            transcript, reference = decoded_output[x][0], target_strings[x][0]\n",
        "            wer_inst = decoder.wer(transcript, reference)\n",
        "            cer_inst = decoder.cer(transcript, reference)\n",
        "            total_wer += wer_inst\n",
        "            total_cer += cer_inst\n",
        "            num_tokens += len(reference.split())\n",
        "            num_chars += len(reference.replace(' ', ''))\n",
        "            if verbose:\n",
        "                print(\"Ref:\", reference.lower())\n",
        "                print(\"Hyp:\", transcript.lower())\n",
        "                print(\"WER:\", float(wer_inst) / len(reference.split()),\n",
        "                      \"CER:\", float(cer_inst) / len(reference.replace(' ', '')), \"\\n\")\n",
        "    wer = float(total_wer) / num_tokens\n",
        "    cer = float(total_cer) / num_chars\n",
        "    return wer * 100, cer * 100, output_data"
      ],
      "metadata": {
        "id": "OZXYoUdzW5UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inference"
      ],
      "metadata": {
        "id": "x_dDNuqgW6Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def transcribe(audio_path, spect_parser, model, decoder, device, use_half):\n",
        "    spect = spect_parser.parse_audio(audio_path).contiguous()\n",
        "    spect = spect.view(1, 1, spect.size(0), spect.size(1))\n",
        "    spect = spect.to(device)\n",
        "    if use_half:\n",
        "        spect = spect.half()\n",
        "    input_sizes = torch.IntTensor([spect.size(3)]).int()\n",
        "    out, output_sizes = model(spect, input_sizes)\n",
        "    decoded_output, decoded_offsets = decoder.decode(out, output_sizes)\n",
        "    return decoded_output, decoded_offsets"
      ],
      "metadata": {
        "id": "Ld3el53BW8Oz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}